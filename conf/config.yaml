# Training Configuration

training:
  num_epochs: 50
  batch_size: 32

optimizer:
  finetune:
    lr_new_layers: 0.05
    lr_pretrained_layers: 0.0007
    momentum: 0.8
  scratch:
    lr: 0.02
    momentum: 0.8

scheduler:
  step_size: 7
  gamma: 0.1